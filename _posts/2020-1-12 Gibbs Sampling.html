<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>2020-1-12 Gibbs Sampling</title></head>
<body><h1>引言</h1>
<p>由于在标签伯努利滤波器中，为了避免传播会产生弱更新组件的预测组件，提出了GLMB过滤器的一个新实现，该实现具有联合预测和更新功能。这种联合策略每次迭代只需要一个截断过程，同时保持了滤波性能和并行性。进一步使用，δGLMB截断方法基于吉布斯采样,大大降低了复杂性，并且误差在可接受的范围内。以后就是对吉布斯采样的一些理解供大家参考。</p>
<h1>随机采样</h1>
<p>吉布斯采样可以说是MC（蒙特卡洛）采样的一种延伸发展，蒙特卡洛起源于那个著名的投针实验（可自行了解）。随着计算机的发展，随机采样技术进入了人们的视野中，对于那些不可行或者用原有方法无法解决的问题，蒙特卡洛方法给予了人们一种思路供人选择。</p>
<p>MC可以解决一些低维度的问题，给定一个概率分布p(x),计算机通过计算生成它的样本。但是当概率分布p(x)是一个比较好得到的分布的话，确实可以得到样本。但是当p(x)是一个高维的分布时，样本就很困难。因此提出一种新的更为复杂的随机模拟方法去框定一个p(x)生成样本，便是MCMC算法。</p>
<h1>基于markov Chain的MC算法</h1>
<p>在介绍MCMC算法之前，先介绍一些前置概念。</p>
<p>马尔科夫链：t时刻的状态只取决于t-1时刻的状态（过于基础自行脑补）。</p>
<p>马尔科夫收敛定理1：如果一个非周期马氏链具有转移概率矩阵P，且它的任何两个状态是连通的，那么$\lim_{n\rightarrow\infty}P^n_{ij}$ 存在且与$i$ 无关。记$\lim_{n\rightarrow\infty}P^n_{ij}=\pi(j)$ ,得到：</p>
<div>$$
1.\lim_{n\rightarrow\infty}P^n=
\begin{bmatrix}
\pi(1)&amp;\pi(2)...&amp;\pi(j)...\\
\pi(1)&amp;\pi(2)...&amp;\pi(j)...\\
....&amp;...&amp;...\\
\pi(1)&amp;\pi(2)...&amp;\pi(j)...\\
...&amp;...&amp;...
\end{bmatrix}
$$</div>
<p>2.$\pi(j)=\sum^\infty_{i=0}\pi(i)P_{ij}$ </p>
<p>3.π是方程πP=π的唯一非负解，其中，</p>
<div>$$
\pi=[\pi(1),\pi(2),...,\pi(j),...],\sum^\infty_{i=0}\pi_i=1
$$</div>
<p>π就成为马氏链的平稳分布。</p>
<p>意思就是说一开始你设定的初始值不决定你最后的平稳分布，决定平稳分布的是转移概率矩阵。无论如何，经过n步的收敛，最后得到的都是平稳分布的样本。</p>
<h2>Metropolis算法</h2>
<p> 给定概率分布p(x)，我们希望有便捷的方式生成它对应的样本，由于马氏链能收敛到平稳分布，那么：<strong><em>如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x)，那么我们从任何一个初始状态出发沿着马氏链转移，得到一个转移序列，如果马氏链在第n步收敛了，那么我们取n步以后的转移序列，就得到了平稳分布p(x)的样本</em>。</strong>在平稳分布之前的过程称之为burn-in。在学术paper中，不需要考虑burn-in过程，在工程中就需要考虑burn-in这一工程的速率一类的问题。</p>
<p>   Metropolis基于这个绝妙的想法，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现，Metropolis算法是首个普适的采样方法，并启发了一系列的MCMC方法，所以人们视他为随机模拟技术腾飞的起点。</p>
<h2>MCMC算法</h2>
<p>MCMC是Metropolis算法的一种改进，它用到一个一种新的定理，叫做马氏链收敛定理2.</p>
<p>如果非周期马氏链的转移矩阵P和分布π（x）满足</p>
<div>$$
\pi(i)P_{ij}=\pi(j)P_{ij} \\for \ all \ i,j
$$</div>
<p>则π（x）是马氏链的平稳分布，这就叫做细致平稳条件。</p>
<p>我们可以这么理解这个定理，两个状态i和j，从i到j的丢掉的东西必须到等于从j回到i所获取的东西，只有这样状态i的概率质量π（i）才是稳定的，才能等到平稳分布。</p>
<p>这样我们接下来的工作就是将i和j的工程两端配平，仅此引入一个新的量α（i，j）。</p>
<div>$$
p(i)\underbrace{q(i,j)\alpha(i,j)}_{Q&#39;(i,j)}=p(j)\underbrace{q(j,i)\alpha(j,i)}_{Q&#39;(j,i)}
$$</div>
<p>这里面，q为从一个状态到另一个状态的概率，因为通常情况下细致平稳条件不成立因此因此α。这时我们便可以得到最后的平稳分布p(x).( 马氏链Q‘（i,j）=q(i,j)α(i,j) )。α我们称之为接受率，1-α(i,j)为拒绝转移，α(i,j)为接受转移。</p>
<p>这里面出现了一个问题，当马氏链Q在转移中的接受率太小的时候，采样过程容易原地踏步，拒绝了太多的转移，这会使得这个遍历过程过于的繁琐过长，收敛速度受到了很大的影响，因此需要提高接受率。</p>
<p><img src="/public/image/tran.jpg" referrerpolicy="no-referrer" alt="tran"></p>
<h2>Metropolis-Hastings算法</h2>
<p>很简单，对接受率进行一些微调，将式子两端同乘以一个数，将两边扩大几倍。既没有打破细致平稳条件，又提高了接受率，但是！对于高维的问题，这还不够，必须找到一个转移矩阵Q让接受率为1才行。</p>
<div>$$
\alpha(i,j)=min\{\frac{p(j)q(j,i)}{p(i)q(i,j)},1\}
$$</div>
<h2>Gibbs Sampling</h2>
<p>(1).先从二维的情形下分析，绛舌一个概率分布p(x,y),考察x坐标相同的两个点，A（x1,y1）,B(x2,y2),我们发现</p>
<div>$$
p(x1,y1).p(y2|x1)=p(x1).p(y1|x1).p(y2|x1)，              p(x1,y2).p(y1|x1)=p(x1).p(y2|x1).p(y1|x1)
$$</div>
<p>所以得到p(x1,y1).p(y2|x1)=p(x1,y2).p(y1|x1),</p>
<p>即p(A)p(y2|x1)=p(B)p(y1|x1).</p>
<p>因此我们发现平行的两点之间的转移满足细致平稳条件。所以我们知道了，在沿着坐标轴方向的转移都是满足细致平稳条件的，这个时候的接受率为1，这样我们就不需要像Metropolis-Hastings一样将不等式变成等式了。这个等式可以理解为两边都乘以了接受率1，等式成立。</p>
<p><img src="/public/image/zhoubiaozhou.jpg" referrerpolicy="no-referrer" alt="zuobiaozhou"></p>
<p>于是我们可以构造平面上任意两点之间的<strong>*转移概率矩阵Q:*</strong></p>
<p>   Q(A-&gt;B)=p(y2|x1)   如果xA=xB=x1</p>
<p>   Q(A-&gt;C)=p(x2|y1)   如果yA=yC=y1</p>
<p>   Q(A-&gt;D)=0         其他</p>
<p>   有了如上的转移概率矩阵Q，我们很容易验证<strong><em>对于平面上任意两点X,Y，满足细致平稳条件</em></strong>：p(X)Q(X-&gt;Y)=p(Y)Q(Y-&gt;X)</p>
<p>  于是这个二维空间上的马氏链将收敛到平稳分布p(x,y)，而这个算法就称为Gibbs Sampling算法。</p>
<p>二维Gibbs Sampling算法流程</p>
<p>1.随机初始化X0=x0，Y0=y0</p>
<p>2.对t=0,1,2...,循环采样</p>
<div>$$
y(t+1)\sim p(y\mid x_t)\\
x(t+1)\sim p(x\mid y_{t+1})
$$</div>
<p>这里再说一句，这种用坐标轴形势的推导是一种简便形式，也可以用自己定的一套坐标轴然后用转移概率对x轴，y轴做转移，最后得到的马氏链也是收敛的。坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵 Q 中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻t，在一根固定的坐标轴上转移的概率是1。</p>
<p>(2)n维Gibbs Sampling</p>
<p>将二维推广到高维，推导过程不变，细致平稳条件同样成立。但只能沿着坐标轴做转移。</p>
<p>概率分布（转移概率，条件概率）变成了$p(x_i\mid x_1,...,x_{i-1},x_{i+1},...,x_n)$ .不能跟着单一一根坐标轴做的跳转，转移概率全部为0.</p>
<p>n维Gibbs Sampling算法流程：</p>
<p>1.随机初始化{$x_i:i=1,2,...,n$}</p>
<p>2.对t=0,1,2，...，循环采样</p>
<div>$$
\begin{aligned}
&amp;1.x_1^{(t+1)\sim p(x_1\mid x_2^{(t)},...,x^{t}_n)}\\
&amp;2.x_2^{(t+1)\sim p(x_1\mid x_3^{(t+1)},...,x^{t}_n)}\\
&amp;3...\\
&amp;4.x_j^{(t+1)\sim p(x_1\mid x_2^{(t+1)},...,x^{t}_{j-1},x^{(t)}_{j+1},...,x^{t}_n)}\\
&amp;5....\\
&amp;6.x^{(t+1)}_n\sim p(x_j\mid x^{(t+1)}_1,x_2^{(t+1)},...,x^{(t+1)}_n)
\end{aligned}
$$</div>
<p>也就是说，gibbs采样时通过条件分布采样模拟联合分布，再通过模拟的联合分布直接推导出条件分布，以此循环。Ps（最后需要归一化）</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1>标签伯努利滤波器中的Gibbs Sampling</h1>
<p>FastGLMB中先运用联合预测更新，预测更新环节是对于参数$I,\omega,p$ 作为指标带入进行计算，而gibbs是通过参数$\gamma^{(h,1)},\eta^{(h)},T_+^{(h)}$作为输入量进行抽样，gamma作为初始值，eta作为转移概率最后抽样出关于gamma的样本，从而带入联合预测和更新中。Gibbs可以降低整个预测更新的复杂度，一切都是为了避免弱小组件进入循环计算。</p>
<p><img src="/public/image/joint.jpg" referrerpolicy="no-referrer" alt="joint"></p>
<p><img src="/public/image/gibbs.jpg" referrerpolicy="no-referrer" alt="gibbs"></p>
</body>
</html>